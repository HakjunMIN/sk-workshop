{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Native Functions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "Two of the previous notebooks showed how to [execute semantic functions inline](./03-semantic-function-inline.ipynb) and how to [run prompts from a file](./02-running-prompts-from-file.ipynb).\n",
    "\n",
    "In this notebook, we'll show how to use native functions from a file. We will also show how to call semantic functions from native functions.\n",
    "\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "- Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "- Using external data sources to gather data to concatenate into your prompt.\n",
    "- Validating user input data prior to sending it to the LLM prompt.\n",
    "\n",
    "Native functions are defined using standard Python code. The structure is simple, but not well documented at this point.\n",
    "\n",
    "The following examples are intended to help guide new users towards successful native & semantic function use with the SK Python framework.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f726252",
   "metadata": {},
   "source": [
    "Initial configuration for the notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecfe74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure paths are correct for the imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a888bb7",
   "metadata": {},
   "source": [
    "We will load our settings and get the LLM service to use for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fddb5403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "from services import Service\n",
    "\n",
    "from service_settings import ServiceSettings\n",
    "\n",
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee3dc1",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd150646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "            env_file_path=\"../.env\",\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            env_file_path=\"../.env\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "186767f8",
   "metadata": {},
   "source": [
    "Let's create a **native** function that gives us a random number between 3 and a user input as the upper limit. We'll use this number to create 3-x paragraphs of text when passed to a semantic function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "First, let's create our native function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\",\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(3, int(input)))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Next, let's create a semantic function that accepts a number as `{{$input}}` and generates that number of paragraphs about two Corgis on an adventure. `$input` is a default variable semantic functions can use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7890943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "\n",
    "prompt = \"\"\"\n",
    "모험을 떠나는 두 웰시코기들의 짧은 이야기를 작성하세요.\n",
    "스토리가 있어야 합니다:\n",
    "- G 등급\n",
    "- 긍정적인 메시지가 있어야 합니다.\n",
    "- 성차별, 인종차별 또는 기타 편견/편견이 없어야 합니다.\n",
    "- 정확히 {{$input}} 단락 길이여야 합니다. 이 길이여야 합니다.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-35-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"story\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.add_function(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "generate_number_plugin = kernel.add_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2471c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_plugin[\"GenerateNumberThreeOrHigher\"]\n",
    "\n",
    "number_result = await generate_number_three_or_higher(kernel, input=6)\n",
    "print(number_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f043a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = await corgi_story.invoke(kernel, input=number_result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245e7a2",
   "metadata": {},
   "source": [
    "_Note: depending on which model you're using, it may not respond with the proper number of paragraphs._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59a60e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 6 paragraphs long.\n",
      "=====================================================\n",
      "옛날 옛적에, 푸른 들판과 아름다운 숲으로 둘러싸인 작은 마을에 두 마리의 웰시코기 강아지, 루니와 벤지가 살고 있었습니다. 루니는 항상 호기심이 많고 새로운 것을 탐험하기 좋아하는 성격이었고, 벤지는 조용하고 신중한 성격이었지만 루니를 따라가며 모험을 즐겼습니다. 그들은 마을에서 가장 친한 친구로 알려져 있었고, 항상 함께 다니며 서로를 지켜주었습니다.\n",
      "\n",
      "어느 날, 루니와 벤지는 마을 외곽에 있는 신비로운 숲으로 모험을 떠나기로 결심했습니다. 그들은 숲 속 깊숙이 들어가면서 다양한 동물 친구들을 만났습니다. 다람쥐 찰리는 그들에게 가장 맛있는 도토리가 어디에 있는지 알려주었고, 부엉이 올리는 길을 잃지 않도록 높은 나무에서 길을 안내해 주었습니다. 두 강아지는 새로운 친구들을 만나고 도움을 받으며 숲 속을 탐험하는 것이 너무나 즐거웠습니다.\n",
      "\n",
      "숲 속을 걷던 중, 루니와 벤지는 길을 잃고 말았습니다. 벤지는 걱정이 되었지만, 루니는 벤지를 안심시키며 함께 해결책을 찾자고 말했습니다. 그들은 서로를 믿고 협력하며 숲 속에서 빠져나갈 방법을 고민하기 시작했습니다. 두 강아지는 각자의 장점을 살려 문제를 해결하기로 했습니다. 루니는 높은 곳에 올라가 주변을 살펴보고, 벤지는 냄새를 맡아 길을 찾기 시작했습니다.\n",
      "\n",
      "마침내, 벤지는 마을에서 나는 익숙한 냄새를 맡고 그 방향으로 걸음을 옮기기 시작했습니다. 루니는 벤지를 따라가며 조용히 길을 잃지 않도록 신경 썼습니다. 그들은 서로의 노력을 격려하며 조금씩 마을로 돌아가는 길을 찾아갔습니다. 결국 두 강아지는 힘을 합쳐 무사히 마을로 돌아올 수 있었습니다.\n",
      "\n",
      "마을로 돌아온 루니와 벤지는 자신들이 겪은 모험 이야기를 마을 친구들에게 들려주었습니다. 그들은 서로를 믿고 협력하면 어떤 어려움도 이겨낼 수 있다는 중요한 교훈을 배웠습니다. 마을 친구들은 두 강아지의 이야기를 들으며 큰 박수를 쳤고, 루니와 벤지는 자신들의 모험이 다른 친구들에게도 희망과 용기를 줄 수 있어 기뻤습니다.\n",
      "\n",
      "그 후로도 루니와 벤지는 많은 모험을 함께하며 더욱 단단한 우정을 쌓아갔습니다. 그들은 항상 서로를 응원하고 도와주며, 마을 친구들에게 긍정적인 영향을 끼쳤습니다. 루니와 벤지의 이야기는 오랫동안 마을에서 전해졌고, 많은 이들에게 용기와 희망을 주는 아름다운 전설로 남게 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story exactly {number_result.value} paragraphs long.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef29d16",
   "metadata": {},
   "source": [
    "## Kernel Functions with Annotated Parameters\n",
    "\n",
    "That works! But let's expand on our example to make it more generic.\n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of the Python's `Annotated` class to hold these variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d54983d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "091f45e4",
   "metadata": {},
   "source": [
    "Let's start with the native function. Notice that we're add the `@kernel_function` decorator that holds the name of the function as well as an optional description. The input parameters are configured as part of the function's signature, and we use the `Annotated` type to specify the required input arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ea462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Annotated\n",
    "\n",
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNumberPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @kernel_function(\n",
    "        name=\"GenerateNumber\",\n",
    "        description=\"Generate a random number between min and max\",\n",
    "    )\n",
    "    def generate_number(\n",
    "        self,\n",
    "        min: Annotated[int, \"the minimum number of paragraphs\"],\n",
    "        max: Annotated[int, \"the maximum number of paragraphs\"] = 10,\n",
    "    ) -> Annotated[int, \"the output is a number\"]:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(min, max))\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {min} and {max}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48bcdf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "generate_number_plugin = kernel.add_plugin(GenerateNumberPlugin(), \"GenerateNumberPlugin\")\n",
    "generate_number = generate_number_plugin[\"GenerateNumber\"]\n",
    "result = await generate_number(kernel, min=3, max=8)\n",
    "print (result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad068d6",
   "metadata": {},
   "source": [
    "Now let's also allow the semantic function to take in additional arguments. In this case, we're going to allow the our CorgiStory function to be written in a specified language. We'll need to provide a `paragraph_count` and a `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b8286fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "모험을 떠나는 두 웰시코기들의 짧은 이야기를 작성하세요.\n",
    "스토리가 있어야 합니다:\n",
    "- G 등급\n",
    "- 긍정적인 메시지가 있어야 합니다.\n",
    "- 성차별, 인종차별 또는 기타 편견/편견이 없어야 합니다.\n",
    "- 정확히 {{$parameter_count}} 문단 길이여야 합니다.\n",
    "- 이 언어로 작성해야 합니다: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"summarize\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.add_function(\n",
    "    function_name=\"CorgiStory\",\n",
    "    plugin_name=\"CorgiPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8778bad",
   "metadata": {},
   "source": [
    "Let's generate a paragraph count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28820d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 5 paragraphs long.\n"
     ]
    }
   ],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value\n",
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a9147",
   "metadata": {},
   "source": [
    "We can now invoke our corgi_story function using the `kernel` and the keyword arguments `paragraph_count` and `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbe07c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variable `Symbols.VAR_PREFIX: parameter_count` not found in the KernelArguments\n"
     ]
    }
   ],
   "source": [
    "# Pass the output to the semantic story function\n",
    "desired_language = \"Japanese\"\n",
    "story = await corgi_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6732a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 5 paragraphs long in Japanese.\n",
      "=====================================================\n",
      "ある晴れた日の朝、二匹のウェルシュコーギー、ポポとモモは大冒険に出かけることにしました。彼らはいつも一緒に遊んでいましたが、今日は特別な日です。ポポとモモは近くの森に向かい、そこで新しい友達を見つけることが目的でした。森の中で彼らは迷子になった小さなリス、リリーを発見しました。リリーは家に帰る道がわからず、心細そうにしていました。ポポとモモは力を合わせてリリーを元気づけ、彼女の家まで案内しました。リリーは感謝の気持ちでいっぱいになり、ポポとモモに美味しいどんぐりをプレゼントしました。この冒険を通じて、ポポとモモは助け合うことの大切さと、新しい友達を作る喜びを学びました。彼らは幸せな気持ちでいっぱいになり、また新たな冒険を夢見ながら森を後にしました。\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb786c54",
   "metadata": {},
   "source": [
    "## Calling Native Functions within a Semantic Function\n",
    "\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Prompt Functions!\n",
    "\n",
    "We will make our CorgiStory semantic function call a native function `GenerateNames` which will return names for our Corgi characters.\n",
    "\n",
    "We do this using the syntax `{{plugin_name.function_name}}`. You can read more about our prompte templating syntax [here](../../../docs/PROMPT_TEMPLATE_LANGUAGE.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d84c7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import kernel_function\n",
    "\n",
    "\n",
    "class GenerateNamesPlugin:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @kernel_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @kernel_function(description=\"Generate character names\", name=\"generate_names\")\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\"Hoagie\", \"Hamilton\", \"Bacon\", \"Pizza\", \"Boots\", \"Shorts\", \"Tuna\"}\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ab7d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_plugin = kernel.add_plugin(GenerateNamesPlugin(), plugin_name=\"GenerateNames\")\n",
    "generate_names = generate_names_plugin[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94decd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "모험을 떠나는 두 웰시코기들의 짧은 이야기를 작성하세요.\n",
    "스토리가 있어야 합니다:\n",
    "- G 등급\n",
    "- 긍정적인 메시지가 있어야 합니다.\n",
    "- 성차별, 인종차별 또는 기타 편견/편견이 없어야 합니다.\n",
    "- 정확히 {{$parameter_count}} 문단 길이여야 합니다.\n",
    "- 이 언어로 작성해야 합니다: {{$language}}\n",
    "- 코기의 두 가지 이름은 {{GenerateNames.generate_names}}입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be72a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"corgi-new\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"paragraph_count\", description=\"The number of paragraphs\", is_required=True),\n",
    "        InputVariable(name=\"language\", description=\"The language of the story\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "corgi_story = kernel.add_function(\n",
    "    function_name=\"CorgiStoryUpdated\",\n",
    "    plugin_name=\"CorgiPluginUpdated\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56e6cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await generate_number.invoke(kernel, min=1, max=5)\n",
    "num_paragraphs = result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e980348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Variable `Symbols.VAR_PREFIX: parameter_count` not found in the KernelArguments\n"
     ]
    }
   ],
   "source": [
    "desired_language = \"English\"\n",
    "story = await corgi_story.invoke(kernel, paragraph_count=num_paragraphs, language=desired_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4ade048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story 1 paragraphs long in English.\n",
      "=====================================================\n",
      "Once upon a time, two adventurous Welsh Corgis named Tuna and Bacon set out on a grand journey through the enchanted forest near their home. They were best friends and always stayed by each other’s side. The forest was filled with tall trees, sparkling streams, and friendly woodland creatures. As they traveled, they came across a baby bird that had fallen from its nest. Tuna and Bacon worked together, using their keen senses and gentle paws, to carefully place the baby bird back into its nest. The mother bird chirped joyfully, and the two Corgis felt an immense sense of accomplishment. They continued their journey, helping anyone in need, and spreading kindness wherever they went. By the end of their adventure, Tuna and Bacon learned that the greatest treasure of all is the joy that comes from helping others. They returned home with their hearts full, ready for their next adventure, knowing that together, they could overcome any challenge.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating a corgi story {num_paragraphs} paragraphs long in {desired_language}.\")\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42f0c472",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A quick review of what we've learned here:\n",
    "\n",
    "- We've learned how to create native and prompt functions and register them to the kernel\n",
    "- We've seen how we can use Kernel Arguments to pass in more custom variables into our prompt\n",
    "- We've seen how we can call native functions within a prompt.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

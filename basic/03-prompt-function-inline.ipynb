{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c93ac5b",
   "metadata": {},
   "source": [
    "# Running Prompt Functions Inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704e54b",
   "metadata": {},
   "source": [
    "Initial configuration for the notebook to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebb95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure paths are correct for the imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "\n",
    "sys.path.append(grandparent_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40201641",
   "metadata": {},
   "source": [
    "The [previous notebook](./02-running-prompts-from-file.ipynb)\n",
    "showed how to define a semantic function using a prompt template stored on a file.\n",
    "\n",
    "In this notebook, we'll show how to use the Semantic Kernel to define functions inline with your python code. This can be useful in a few scenarios:\n",
    "\n",
    "- Dynamically generating the prompt using complex rules at runtime\n",
    "- Writing prompts by editing Python code instead of TXT files.\n",
    "- Easily creating demos, like this document\n",
    "\n",
    "Prompt templates are defined using the SK template language, which allows to reference variables and functions. Read [this doc](https://aka.ms/sk/howto/configurefunction) to learn more about the design decisions for prompt templating.\n",
    "\n",
    "For now we'll use only the `{{$input}}` variable, and see more complex templates later.\n",
    "\n",
    "Almost all semantic function prompts have a reference to `{{$input}}`, which is the default way\n",
    "a user can import content from the context variables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d90b0c13",
   "metadata": {},
   "source": [
    "Prepare a semantic kernel instance first, loading also the AI service settings defined in the [Setup notebook](00-getting-started.ipynb):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377f42e",
   "metadata": {},
   "source": [
    "Let's define our kernel for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462b281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "kernel = Kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734d121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using service type: Service.AzureOpenAI\n"
     ]
    }
   ],
   "source": [
    "from services import Service\n",
    "\n",
    "from service_settings import ServiceSettings\n",
    "\n",
    "service_settings = ServiceSettings.create()\n",
    "\n",
    "# Select a service to use for this notebook (available services: OpenAI, AzureOpenAI, HuggingFace)\n",
    "selectedService = (\n",
    "    Service.AzureOpenAI\n",
    "    if service_settings.global_llm_service is None\n",
    "    else Service(service_settings.global_llm_service.lower())\n",
    ")\n",
    "print(f\"Using service type: {selectedService}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06740170",
   "metadata": {},
   "source": [
    "We now configure our Chat Completion service on the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3712b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all services so that this cell can be re-run without restarting the kernel\n",
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "            env_file_path=\"../.env\",\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            env_file_path=\"../.env\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589733c5",
   "metadata": {},
   "source": [
    "Let's use a prompt to create a semantic function used to summarize content, allowing for some creativity and a sufficient number of tokens.\n",
    "\n",
    "The function will take in input the text to summarize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae29c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "\n",
    "prompt = \"\"\"{{$input}}\n",
    "Summarize the content above.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-4o\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"summarize\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "summarize = kernel.add_function(\n",
    "    function_name=\"summarizeFunc\",\n",
    "    plugin_name=\"summarizePlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26b90c4",
   "metadata": {},
   "source": [
    "Set up some content to summarize, here's an extract about Demo, an ancient Greek poet, taken from Wikipedia (https://en.wikipedia.org/wiki/Demo_(ancient_Greek_poet)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "314557fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "데모(고대 그리스 시인)\n",
    "무료 백과 사전, 위키 백과에서\n",
    "데모 또는 다모(그리스어: Δεμώ, Δαμώ, 기원전 200년경)는 로마 시대의 그리스 여성으로, 그녀의 이름이 새겨진 멤논의 거상에 새겨진 한 편의 시로 유명합니다. 거기에는 자신을 뮤즈에게 바친 서정 시인으로 묘사하고 있지만, 그녀의 생애에 대해서는 알려진 바가 없습니다.[1]\n",
    "정체성\n",
    "데메테르의 전통적인 별칭인 그녀의 이름에서 알 수 있듯이 데모는 그리스인임이 분명합니다. 이 이름은 이집트 등 헬레니즘 세계에서 비교적 흔한 이름이었기 때문에 더 이상 신원을 확인할 수 없습니다. 그녀가 멤논의 거상을 방문한 날짜는 확실하게 알 수 없지만, 왼쪽 다리에 새겨진 내부 증거에 따르면 서기 196년 이후 어느 시점에 그곳에 시가 새겨진 것으로 보입니다.[2]\n",
    "에피그램\n",
    "멤논의 거상에는 여러 개의 낙서 비문이 새겨져 있습니다. 줄리아 발빌라가 쓴 세 개의 에피그램에 이어 네 번째 에피그램은 “데모” 또는 “다모”(그리스어 비문은 읽기 어려움)가 쓴 것으로 추정되는 우아한 커플트로, 뮤즈에게 헌정되었습니다.[2] 이 시는 전통적으로 발빌라의 작품으로 출판되었지만 내부 증거는 다른 저자를 시사합니다.[1]\n",
    "이 시에서 데모는 멤논이 자신에게 특별한 존경심을 보였다고 설명합니다. 이에 대한 보답으로 데모는 영웅에게 시를 선물합니다. 이 에피그램의 마지막에서 그녀는 멤논을 언급하며 그의 힘과 거룩함을 회상하며 그의 신성한 지위를 강조합니다.[2]\n",
    "데모는 줄리아 발빌라와 마찬가지로 인공적이고 시적인 에올리아 방언으로 글을 씁니다. 이 언어는 그녀가 호메로스의 시를 잘 알고 있었음을 나타냅니다. 예를 들어 '즐거운 선물을 안고'라는 문구는 일리아드와 오디세이 전반에 걸쳐 이 문구가 사용되었음을 암시합니다.[a][2][3]\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf0f2330",
   "metadata": {},
   "source": [
    "...and run the summary function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b0e3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데모 또는 다모는 기원전 200년경의 로마 시대 그리스 여성 시인으로, 멤논의 거상에 새겨진 시로 유명합니다. 그녀는 뮤즈에게 자신을 바친 서정 시인으로 묘사되지만, 생애에 대해서는 거의 알려진 바가 없습니다. 데모는 그리스인으로 추정되며, 멤논의 거상을 방문하여 시를 새긴 시점은 서기 196년 이후로 보입니다. 멤논의 거상에는 여러 낙서 비문이 있는데, 데모의 시는 뮤즈에게 헌정된 우아한 커플트로 구성되어 있습니다. 이 시는 멤논의 신성한 지위를 강조하며, 데모는 호메로스의 시를 잘 알고 있었음을 나타내는 에올리아 방언을 사용했습니다.\n"
     ]
    }
   ],
   "source": [
    "summary = await kernel.invoke(summarize, input=input_text)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c2c1262",
   "metadata": {},
   "source": [
    "# Using ChatCompletion for Semantic Plugins\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29b59b28",
   "metadata": {},
   "source": [
    "You can also use chat completion models (like `gpt-35-turbo` and `gpt4`) for creating plugins. Normally you would have to tweak the API to accommodate for a system and user role, but SK abstracts that away for you by using `kernel.add_service` and `AzureChatCompletion` or `OpenAIChatCompletion`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4777f447",
   "metadata": {},
   "source": [
    "Here's one more example of how to write an inline Semantic Function that gives a TLDR for a piece of text using a ChatCompletion model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5886aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.remove_all_services()\n",
    "\n",
    "service_id = None\n",
    "if selectedService == Service.OpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        OpenAIChatCompletion(\n",
    "            service_id=service_id,\n",
    "            env_file_path=\"../.env\",\n",
    "        ),\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "    service_id = \"default\"\n",
    "    kernel.add_service(\n",
    "        AzureChatCompletion(\n",
    "            service_id=service_id,\n",
    "            env_file_path=\"../.env\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea8128c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 로봇은 인간에게 해를 끼치지 않고, 인간의 명령을 따르되, 자신의 존재를 보호해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "\n",
    "prompt = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "30단어 이내로 TLDR을 알려주세요.\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "    1) 로봇은 사람에게 상해를 입히거나 부주의로 인해\n",
    "    인간을 위험에 빠뜨릴 수 없습니다.\n",
    "\n",
    "    2) 로봇은 다음과 같은 경우를 제외하고는 인간이 내린 명령에 복종해야 합니다.\n",
    "    제 1 법칙과 상충되는 경우를 제외하고는 로봇은 인간의 명령을 따라야 합니다.\n",
    "\n",
    "    3) 로봇은 자신의 존재를 보호해야 하며, 그러한 보호가 제1법칙 또는 제2법칙과 충돌하지 않는 한\n",
    "    제1법칙 또는 제2법칙과 충돌하지 않는 한 로봇은 자신의 존재를 보호해야 합니다.\n",
    "\"\"\"\n",
    "\n",
    "if selectedService == Service.OpenAI:\n",
    "    execution_settings = OpenAIChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        ai_model_id=\"gpt-3.5-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "elif selectedService == Service.AzureOpenAI:\n",
    "    execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=service_id,\n",
    "        # ai_model_id=\"gpt-35-turbo\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "prompt_template_config = PromptTemplateConfig(\n",
    "    template=prompt,\n",
    "    name=\"tldr\",\n",
    "    template_format=\"semantic-kernel\",\n",
    "    input_variables=[\n",
    "        InputVariable(name=\"input\", description=\"The user input\", is_required=True),\n",
    "    ],\n",
    "    execution_settings=execution_settings,\n",
    ")\n",
    "\n",
    "tldr_function = kernel.add_function(\n",
    "    function_name=\"tldrFunction\",\n",
    "    plugin_name=\"tldrPlugin\",\n",
    "    prompt_template_config=prompt_template_config,\n",
    ")\n",
    "\n",
    "summary = await kernel.invoke(tldr_function, input=text)\n",
    "\n",
    "print(f\"Output: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
